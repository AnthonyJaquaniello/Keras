{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from keras import Sequential\n",
    "from sklearn.preprocessing import LabelEncoder #pour le one_hot_encoding.\n",
    "from keras.layers import Dense, Flatten, TimeDistributed #Flatten pour mettre à plat une matrice.\n",
    "from keras import Input, Model\n",
    "from keras.layers import add, Activation #add pour sommer les couches\n",
    "from keras.layers import Conv1D, AveragePooling1D #pour le pooling et la convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction de structure secondaire par réseau de convolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Processing des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction des seq\n",
    "x_train = []\n",
    "\n",
    "with open(\"../data/train.fasta\", \"r\") as file:\n",
    "    for line in file:\n",
    "        x_train.append(list(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['K',\n",
       " 'K',\n",
       " 'V',\n",
       " 'K',\n",
       " 'V',\n",
       " 'S',\n",
       " 'H',\n",
       " 'R',\n",
       " 'S',\n",
       " 'H',\n",
       " 'S',\n",
       " 'T',\n",
       " 'E',\n",
       " 'P',\n",
       " 'G',\n",
       " 'L',\n",
       " 'V',\n",
       " 'L',\n",
       " 'T',\n",
       " 'L',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'G',\n",
       " 'D',\n",
       " 'V',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'G',\n",
       " 'L',\n",
       " 'G',\n",
       " 'E',\n",
       " 'N',\n",
       " 'V',\n",
       " 'M',\n",
       " 'E',\n",
       " 'R',\n",
       " 'K',\n",
       " 'K',\n",
       " 'P',\n",
       " 'A',\n",
       " 'L',\n",
       " 'V',\n",
       " 'S',\n",
       " 'I',\n",
       " 'P',\n",
       " 'E',\n",
       " 'D',\n",
       " 'V',\n",
       " 'V',\n",
       " 'Q',\n",
       " 'A',\n",
       " 'E',\n",
       " 'A',\n",
       " 'G',\n",
       " 'G',\n",
       " 'M',\n",
       " 'H',\n",
       " 'T',\n",
       " 'V',\n",
       " 'C',\n",
       " 'L',\n",
       " 'S',\n",
       " 'K',\n",
       " 'S',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'S',\n",
       " 'F',\n",
       " 'G',\n",
       " 'C',\n",
       " 'N',\n",
       " 'D',\n",
       " 'E',\n",
       " 'G',\n",
       " 'A',\n",
       " 'L',\n",
       " 'G',\n",
       " 'R',\n",
       " 'D',\n",
       " 'T',\n",
       " 'S',\n",
       " 'V',\n",
       " 'E',\n",
       " 'G',\n",
       " 'S',\n",
       " 'E',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " 'G',\n",
       " 'K',\n",
       " 'V',\n",
       " 'E',\n",
       " 'L',\n",
       " 'Q',\n",
       " 'E',\n",
       " 'K',\n",
       " 'V',\n",
       " 'V',\n",
       " 'Q',\n",
       " 'V',\n",
       " 'S',\n",
       " 'A',\n",
       " 'G',\n",
       " 'D',\n",
       " 'S',\n",
       " 'H',\n",
       " 'T',\n",
       " 'A',\n",
       " 'A',\n",
       " 'L',\n",
       " 'T',\n",
       " 'D',\n",
       " 'D',\n",
       " 'G',\n",
       " 'R',\n",
       " 'V',\n",
       " 'F',\n",
       " 'L',\n",
       " 'W',\n",
       " 'G',\n",
       " 'S',\n",
       " 'F',\n",
       " 'R',\n",
       " 'D',\n",
       " 'N',\n",
       " 'N',\n",
       " 'G',\n",
       " 'V',\n",
       " 'I',\n",
       " 'G',\n",
       " 'L',\n",
       " 'L',\n",
       " 'E',\n",
       " 'P',\n",
       " 'M',\n",
       " 'K',\n",
       " 'K',\n",
       " 'S',\n",
       " 'M',\n",
       " 'V',\n",
       " 'P',\n",
       " 'V',\n",
       " 'Q',\n",
       " 'V',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'D',\n",
       " 'V',\n",
       " 'P',\n",
       " 'V',\n",
       " 'V',\n",
       " 'K',\n",
       " 'V',\n",
       " 'A',\n",
       " 'S',\n",
       " 'G',\n",
       " 'N',\n",
       " 'D',\n",
       " 'H',\n",
       " 'L',\n",
       " 'V',\n",
       " 'M',\n",
       " 'L',\n",
       " 'T',\n",
       " 'A',\n",
       " 'D',\n",
       " 'G',\n",
       " 'D',\n",
       " 'L',\n",
       " 'Y',\n",
       " 'T',\n",
       " 'L',\n",
       " 'G',\n",
       " 'C',\n",
       " 'G',\n",
       " 'E',\n",
       " 'Q',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'G',\n",
       " 'R',\n",
       " 'V',\n",
       " 'P',\n",
       " 'E',\n",
       " 'L',\n",
       " 'F',\n",
       " 'A',\n",
       " 'N',\n",
       " 'R',\n",
       " 'G',\n",
       " 'G',\n",
       " 'R',\n",
       " 'Q',\n",
       " 'G',\n",
       " 'L',\n",
       " 'E',\n",
       " 'R',\n",
       " 'L',\n",
       " 'L',\n",
       " 'V',\n",
       " 'P',\n",
       " 'K',\n",
       " 'C',\n",
       " 'V',\n",
       " 'M',\n",
       " 'L',\n",
       " 'K',\n",
       " 'S',\n",
       " 'R',\n",
       " 'G',\n",
       " 'S',\n",
       " 'R',\n",
       " 'G',\n",
       " 'H',\n",
       " 'V',\n",
       " 'R',\n",
       " 'F',\n",
       " 'Q',\n",
       " 'D',\n",
       " 'A',\n",
       " 'F',\n",
       " 'C',\n",
       " 'G',\n",
       " 'A',\n",
       " 'Y',\n",
       " 'F',\n",
       " 'T',\n",
       " 'F',\n",
       " 'A',\n",
       " 'I',\n",
       " 'S',\n",
       " 'H',\n",
       " 'E',\n",
       " 'G',\n",
       " 'H',\n",
       " 'V',\n",
       " 'Y',\n",
       " 'G',\n",
       " 'F',\n",
       " 'G',\n",
       " 'L',\n",
       " 'S',\n",
       " 'N',\n",
       " 'Y',\n",
       " 'H',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'G',\n",
       " 'T',\n",
       " 'P',\n",
       " 'G',\n",
       " 'T',\n",
       " 'E',\n",
       " 'S',\n",
       " 'C',\n",
       " 'F',\n",
       " 'I',\n",
       " 'P',\n",
       " 'Q',\n",
       " 'N',\n",
       " 'L',\n",
       " 'T',\n",
       " 'S',\n",
       " 'F',\n",
       " 'K',\n",
       " 'N',\n",
       " 'S',\n",
       " 'T',\n",
       " 'K',\n",
       " 'S',\n",
       " 'W',\n",
       " 'V',\n",
       " 'G',\n",
       " 'F',\n",
       " 'S',\n",
       " 'G',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'H',\n",
       " 'H',\n",
       " 'T',\n",
       " 'V',\n",
       " 'C',\n",
       " 'M',\n",
       " 'D',\n",
       " 'S',\n",
       " 'E',\n",
       " 'G',\n",
       " 'K',\n",
       " 'A',\n",
       " 'Y',\n",
       " 'S',\n",
       " 'L',\n",
       " 'G',\n",
       " 'R',\n",
       " 'A',\n",
       " 'E',\n",
       " 'Y',\n",
       " 'G',\n",
       " 'R',\n",
       " 'L',\n",
       " 'G',\n",
       " 'L',\n",
       " 'G',\n",
       " 'E',\n",
       " 'G',\n",
       " 'A',\n",
       " 'E',\n",
       " 'E',\n",
       " 'K',\n",
       " 'S',\n",
       " 'I',\n",
       " 'P',\n",
       " 'T',\n",
       " 'L',\n",
       " 'I',\n",
       " 'S',\n",
       " 'R',\n",
       " 'L',\n",
       " 'P',\n",
       " 'A',\n",
       " 'V',\n",
       " 'S',\n",
       " 'S',\n",
       " 'V',\n",
       " 'A',\n",
       " 'C',\n",
       " 'G',\n",
       " 'A',\n",
       " 'S',\n",
       " 'V',\n",
       " 'G',\n",
       " 'Y',\n",
       " 'A',\n",
       " 'V',\n",
       " 'T',\n",
       " 'K',\n",
       " 'D',\n",
       " 'G',\n",
       " 'R',\n",
       " 'V',\n",
       " 'F',\n",
       " 'A',\n",
       " 'W',\n",
       " 'G',\n",
       " 'M',\n",
       " 'G',\n",
       " 'T',\n",
       " 'N',\n",
       " 'Y',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'G',\n",
       " 'T',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'D',\n",
       " 'E',\n",
       " 'D',\n",
       " 'A',\n",
       " 'W',\n",
       " 'S',\n",
       " 'P',\n",
       " 'V',\n",
       " 'E',\n",
       " 'M',\n",
       " 'M',\n",
       " 'G',\n",
       " 'K',\n",
       " 'Q',\n",
       " 'L',\n",
       " 'E',\n",
       " 'N',\n",
       " 'R',\n",
       " 'V',\n",
       " 'V',\n",
       " 'L',\n",
       " 'S',\n",
       " 'V',\n",
       " 'S',\n",
       " 'S',\n",
       " 'G',\n",
       " 'G',\n",
       " 'Q',\n",
       " 'H',\n",
       " 'T',\n",
       " 'V',\n",
       " 'L',\n",
       " 'L',\n",
       " 'V',\n",
       " 'K',\n",
       " 'D',\n",
       " 'K',\n",
       " 'E',\n",
       " 'Q',\n",
       " 'S',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J',\n",
       " 'J']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion avec des \"0\" jusqu'a 759\n",
    "for seq in x_train:\n",
    "    seq.append(\" \")\n",
    "    while len(seq) != 760:\n",
    "        seq.append(\" \")\n",
    "    x_train = np.array(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n",
      "1348\n"
     ]
    }
   ],
   "source": [
    "print(len(x_train[1]))\n",
    "print(len(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction des ddsp:\n",
    "y_train = []\n",
    "\n",
    "with open(\"../data/train.dssp\", 'r') as file:\n",
    "    for line in file:\n",
    "        y_train.append(list(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion jusqu'a 759:\n",
    "for d in y_train:\n",
    "    d.append(\" \")\n",
    "    while len(d) != 760:\n",
    "        d.append(\" \")\n",
    "    y_train = np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760\n",
      "1348\n"
     ]
    }
   ],
   "source": [
    "print(len(y_train[1]))\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348, 760, 21)\n"
     ]
    }
   ],
   "source": [
    "#traduction des X en one_hot_encoding:\n",
    "all_one_hot_x_train = []\n",
    "\n",
    "for i in range(0,len(x_train)):\n",
    "    classes = LabelEncoder()\n",
    "    integer_encoded = classes.fit_transform(x_train[i])\n",
    "    one_hot_i = keras.utils.to_categorical(integer_encoded, num_classes= 21)  \n",
    "    all_one_hot_x_train.append(one_hot_i)\n",
    "all_one_hot_x_train = np.array(all_one_hot_x_train)\n",
    "print(all_one_hot_x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1348, 760, 4)\n"
     ]
    }
   ],
   "source": [
    "#traduction des Y en one_hot_encoding:\n",
    "all_one_hot_y_train = []\n",
    "\n",
    "for j in range(0,len(y_train)):\n",
    "    classes = LabelEncoder()\n",
    "    integer_encoded = classes.fit_transform(y_train[j])\n",
    "    one_hot_j = keras.utils.to_categorical(integer_encoded, num_classes= 4)\n",
    "    all_one_hot_y_train.append(one_hot_j)\n",
    "all_one_hot_y_train = np.array(all_one_hot_y_train)\n",
    "print(all_one_hot_y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction des seq\n",
    "x_test = []\n",
    "\n",
    "with open(\"../data/blind.fasta\", \"r\") as file:\n",
    "    for line in file:\n",
    "        x_test.append(list(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion avec des \"0\" jusqu'a 759\n",
    "for seq in x_test:\n",
    "    seq.append(\" \")\n",
    "    while len(seq) != 760:\n",
    "        seq.append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "\n",
    "with open(\"../data/blind.dssp\", 'r') as file:\n",
    "    for line in file:\n",
    "        y_test.append(list(line[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#completion jusqu'a 759:\n",
    "for d in y_test:\n",
    "    d.append(\" \")\n",
    "    while len(d) != 760:\n",
    "        d.append(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "print(len(x_test))\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 760, 21)\n"
     ]
    }
   ],
   "source": [
    "#traduction des X en one_hot_encoding:\n",
    "all_one_hot_x_test = []\n",
    "\n",
    "for i in range(0,len(x_test)):\n",
    "    classes = LabelEncoder()\n",
    "    integer_encoded = classes.fit_transform(x_test[i])\n",
    "    one_hot_i = keras.utils.to_categorical(integer_encoded, num_classes = 21)  \n",
    "    all_one_hot_x_test.append(one_hot_i)\n",
    "all_one_hot_x_test = np.array(all_one_hot_x_test)\n",
    "print(all_one_hot_x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(149, 760, 4)\n"
     ]
    }
   ],
   "source": [
    "#traduction des Y en one_hot_encoding:\n",
    "all_one_hot_y_test = []\n",
    "\n",
    "for j in range(0,len(y_test)):\n",
    "    classes = LabelEncoder()\n",
    "    integer_encoded = classes.fit_transform(y_test[j])\n",
    "    one_hot_j = keras.utils.to_categorical(integer_encoded, num_classes = 4)  \n",
    "    all_one_hot_y_test.append(one_hot_j)\n",
    "all_one_hot_y_test = np.array(all_one_hot_y_test)\n",
    "print(all_one_hot_y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Construction du réseau CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv1D(filters=21, kernel_size=3, strides=1, padding= \"same\", activation = \"relu\", kernel_initializer=\"he_normal\"))\n",
    "model.add(Conv1D(filters=21, kernel_size=3, strides=1, padding= \"same\", activation = \"linear\", kernel_initializer=\"he_normal\"))\n",
    "model.add(TimeDistributed(Dense(4,activation = \"softmax\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-5f15418b3570>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/dev/lib/python3.7/site-packages/keras/engine/network.py\u001b[0m in \u001b[0;36msummary\u001b[0;34m(self, line_length, positions, print_fn)\u001b[0m\n\u001b[1;32m   1318\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m             raise ValueError(\n\u001b[0;32m-> 1320\u001b[0;31m                 \u001b[0;34m'This model has not yet been built. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m                 \u001b[0;34m'Build the model first by calling build() '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m                 \u001b[0;34m'or calling fit() with some data. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This model has not yet been built. Build the model first by calling build() or calling fit() with some data. Or specify input_shape or batch_input_shape in the first layer for automatic build. "
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1348 samples, validate on 149 samples\n",
      "Epoch 1/20\n",
      "1348/1348 [==============================] - 8s 6ms/step - loss: 0.5286 - accuracy: 0.8666 - val_loss: 0.2448 - val_accuracy: 0.8811\n",
      "Epoch 2/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2365 - accuracy: 0.8838 - val_loss: 0.2165 - val_accuracy: 0.8924\n",
      "Epoch 3/20\n",
      "1348/1348 [==============================] - 4s 3ms/step - loss: 0.2211 - accuracy: 0.8926 - val_loss: 0.2049 - val_accuracy: 0.8997\n",
      "Epoch 4/20\n",
      "1348/1348 [==============================] - 7s 5ms/step - loss: 0.2160 - accuracy: 0.8954 - val_loss: 0.2032 - val_accuracy: 0.9002\n",
      "Epoch 5/20\n",
      "1348/1348 [==============================] - 4s 3ms/step - loss: 0.2136 - accuracy: 0.8965 - val_loss: 0.2019 - val_accuracy: 0.9005\n",
      "Epoch 6/20\n",
      "1348/1348 [==============================] - 3s 3ms/step - loss: 0.2125 - accuracy: 0.8970 - val_loss: 0.2031 - val_accuracy: 0.8997\n",
      "Epoch 7/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2120 - accuracy: 0.8971 - val_loss: 0.2038 - val_accuracy: 0.8988\n",
      "Epoch 8/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2113 - accuracy: 0.8975 - val_loss: 0.2013 - val_accuracy: 0.9008\n",
      "Epoch 9/20\n",
      "1348/1348 [==============================] - 2s 2ms/step - loss: 0.2109 - accuracy: 0.8979 - val_loss: 0.1989 - val_accuracy: 0.9022\n",
      "Epoch 10/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2107 - accuracy: 0.8981 - val_loss: 0.2003 - val_accuracy: 0.9009\n",
      "Epoch 11/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2104 - accuracy: 0.8981 - val_loss: 0.2025 - val_accuracy: 0.8993\n",
      "Epoch 12/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2103 - accuracy: 0.8981 - val_loss: 0.1986 - val_accuracy: 0.9024\n",
      "Epoch 13/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2100 - accuracy: 0.8983 - val_loss: 0.2021 - val_accuracy: 0.8993\n",
      "Epoch 14/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2102 - accuracy: 0.8982 - val_loss: 0.2037 - val_accuracy: 0.8982\n",
      "Epoch 15/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2099 - accuracy: 0.8982 - val_loss: 0.1987 - val_accuracy: 0.9020\n",
      "Epoch 16/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2099 - accuracy: 0.8984 - val_loss: 0.1999 - val_accuracy: 0.9013\n",
      "Epoch 17/20\n",
      "1348/1348 [==============================] - 4s 3ms/step - loss: 0.2095 - accuracy: 0.8985 - val_loss: 0.1991 - val_accuracy: 0.9016\n",
      "Epoch 18/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2089 - accuracy: 0.8991 - val_loss: 0.1979 - val_accuracy: 0.9023\n",
      "Epoch 19/20\n",
      "1348/1348 [==============================] - 3s 2ms/step - loss: 0.2089 - accuracy: 0.8990 - val_loss: 0.2020 - val_accuracy: 0.8995\n",
      "Epoch 20/20\n",
      "1348/1348 [==============================] - 5s 3ms/step - loss: 0.2087 - accuracy: 0.8990 - val_loss: 0.1998 - val_accuracy: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fb3d833df10>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(all_one_hot_x_train,all_one_hot_y_train,epochs=20, batch_size = 20, validation_data = (all_one_hot_x_test,all_one_hot_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1348/1348 [==============================] - 2s 1ms/step\n",
      "[0.20835754713071206, 0.899299144744873]\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(all_one_hot_x_train, all_one_hot_y_train)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention cependant, ces performaances sont trop élevées, cela doit venir des 0 et des '*' que l'on a ajouté pour compléter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(all_one_hot_x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a 4 classes (colonnes), avec 759 lignes (acides aminés)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tp = 2330\n",
      "tot = 5257\n",
      "ACC:44.322\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(all_one_hot_x_test)\n",
    "\n",
    "tp = 0\n",
    "tn = 0\n",
    "fn = 0\n",
    "fp = 0\n",
    "tot = 0\n",
    "for i in range(len(predictions)):\n",
    "    for j in range(len(predictions[i])):\n",
    "        if all_one_hot_y_test[i,j,3] != 0:\n",
    "            predmax = -1\n",
    "            predict_class = -1\n",
    "            true_class = -1\n",
    "            for k in range(len(predictions[i,j])):\n",
    "                if predmax < predictions[i,j,k]:\n",
    "                    predmax = predictions[i,j,k]\n",
    "                    predict_class=k\n",
    "                if all_one_hot_y_test[i,j,k] == 1.:\n",
    "                    true_class = k\n",
    "            if predict_class == true_class:\n",
    "                tp = tp+1\n",
    "            tot=tot+1\n",
    "print(\"tp =\", tp)\n",
    "print(\"tot =\", tot)\n",
    "print(\"ACC:%5.3f\"%(tp/tot*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Réseau Resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def residual_module(lay_i, n_filters):\n",
    "    \"\"\"\n",
    "    Fonction de création de module.\n",
    "    \"\"\"\n",
    "    save = lay_i #on stocke la première couche dans une variable.\n",
    "    conv_1 = Conv1D(n_filters, (3), padding=\"same\", activation=\"relu\",\n",
    "                    kernel_initializer=\"he_normal\")(lay_i)\n",
    "    conv_2 = Conv1D(n_filters, (3), padding=\"same\", activation=\"linear\",\n",
    "                    kernel_initializer=\"he_normal\")(conv_1)\n",
    "    conc_1 = add([conv_2, save])\n",
    "    output = Activation(\"relu\")(conc_1)\n",
    "\n",
    "    return output\n",
    "\n",
    "def my_model7():\n",
    "    n_residual = 4 #on chaine 4 fois les modules\n",
    "    print(\"Simple residual network with {} modules\".format(n_residual))\n",
    "    inputs = Input(shape=(759, 21))\n",
    "    residual_i = inputs\n",
    "    for _ in range(n_residual):\n",
    "        residual_i = residual_module(residual_i, 20)\n",
    "\n",
    "    gavg_1 = AveragePooling1D((2), strides=(1))(residual_i)\n",
    "    flat_1 = Flatten()(gavg_1)\n",
    "    output = Dense(10, activation=\"softmax\")(flat_1)\n",
    "\n",
    "    model = Model(inputs=inputs, outputs=output)\n",
    "    #Pour l'API fonctionnelle\n",
    "\n",
    "#    plot_model(model, to_file=\"residual.png\",\n",
    "#               show_shapes=True, show_layer_names=True)\n",
    "    print(model.summary())\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
