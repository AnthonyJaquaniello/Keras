{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selon des descriptions de radiographies, le modele va prédire si le cancer du sein est bégnin ou main."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'est une étape déterminante dans l'exactitude du réseau de neurone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0               1000025                5                        1   \n",
      "1               1002945                5                        4   \n",
      "2               1015425                3                        1   \n",
      "3               1016277                6                        8   \n",
      "4               1017023                4                        1   \n",
      "..                  ...              ...                      ...   \n",
      "678              776715                3                        1   \n",
      "679              841769                2                        1   \n",
      "680              888820                5                       10   \n",
      "681              897471                4                        8   \n",
      "682              897471                4                        8   \n",
      "\n",
      "     Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "0                           1                  1                            2   \n",
      "1                           4                  5                            7   \n",
      "2                           1                  1                            2   \n",
      "3                           8                  1                            3   \n",
      "4                           1                  3                            2   \n",
      "..                        ...                ...                          ...   \n",
      "678                         1                  1                            3   \n",
      "679                         1                  1                            2   \n",
      "680                        10                  3                            7   \n",
      "681                         6                  4                            3   \n",
      "682                         8                  5                            4   \n",
      "\n",
      "     Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses  class  \n",
      "0              1                3                1        1      2  \n",
      "1             10                3                2        1      2  \n",
      "2              2                3                1        1      2  \n",
      "3              4                3                7        1      2  \n",
      "4              1                3                1        1      2  \n",
      "..           ...              ...              ...      ...    ...  \n",
      "678            2                1                1        1      2  \n",
      "679            1                1                1        1      2  \n",
      "680            3                8               10        2      4  \n",
      "681            4               10                6        1      4  \n",
      "682            5               10                4        1      4  \n",
      "\n",
      "[683 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "names = ['Sample code number', 'Clump Thickness', 'Uniformity of Cell Size',\n",
    "         'Uniformity of Cell Shape','Marginal Adhesion',\n",
    "         'Single Epithelial Cell Size','Bare Nuclei',\n",
    "         'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'class']\n",
    "dataset = pd.read_csv(\"../data/breast-cancer-wisconsin.data\",names=names)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pour que scykitlearn ne plante pas à cause de données non numériques\n",
    "output_check = dataset.applymap(np.isreal).all(0)\n",
    "for i in output_check:\n",
    "    if (i == False):\n",
    "        print(\"Error\")\n",
    "        print(output_check)\n",
    "        quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "678    0.0\n",
       "679    0.0\n",
       "680    1.0\n",
       "681    1.0\n",
       "682    1.0\n",
       "Name: class-value, Length: 683, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"class\"] = dataset[\"class\"].astype(\"category\")\n",
    "dataset[\"class-value\"] = dataset[\"class\"].cat.codes\n",
    "#conversion en float\n",
    "dataset[\"class-value\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(683, 12)\n",
      "    Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "0              1000025                5                        1   \n",
      "1              1002945                5                        4   \n",
      "2              1015425                3                        1   \n",
      "3              1016277                6                        8   \n",
      "4              1017023                4                        1   \n",
      "5              1017122                8                       10   \n",
      "6              1018099                1                        1   \n",
      "7              1018561                2                        1   \n",
      "8              1033078                2                        1   \n",
      "9              1033078                4                        2   \n",
      "10             1035283                1                        1   \n",
      "11             1036172                2                        1   \n",
      "12             1041801                5                        3   \n",
      "13             1043999                1                        1   \n",
      "14             1044572                8                        7   \n",
      "15             1047630                7                        4   \n",
      "16             1048672                4                        1   \n",
      "17             1049815                4                        1   \n",
      "18             1050670               10                        7   \n",
      "19             1050718                6                        1   \n",
      "\n",
      "    Uniformity of Cell Shape  Marginal Adhesion  Single Epithelial Cell Size  \\\n",
      "0                          1                  1                            2   \n",
      "1                          4                  5                            7   \n",
      "2                          1                  1                            2   \n",
      "3                          8                  1                            3   \n",
      "4                          1                  3                            2   \n",
      "5                         10                  8                            7   \n",
      "6                          1                  1                            2   \n",
      "7                          2                  1                            2   \n",
      "8                          1                  1                            2   \n",
      "9                          1                  1                            2   \n",
      "10                         1                  1                            1   \n",
      "11                         1                  1                            2   \n",
      "12                         3                  3                            2   \n",
      "13                         1                  1                            2   \n",
      "14                         5                 10                            7   \n",
      "15                         6                  4                            6   \n",
      "16                         1                  1                            2   \n",
      "17                         1                  1                            2   \n",
      "18                         7                  6                            4   \n",
      "19                         1                  1                            2   \n",
      "\n",
      "    Bare Nuclei  Bland Chromatin  Normal Nucleoli  Mitoses class  class-value  \n",
      "0             1                3                1        1     2            0  \n",
      "1            10                3                2        1     2            0  \n",
      "2             2                3                1        1     2            0  \n",
      "3             4                3                7        1     2            0  \n",
      "4             1                3                1        1     2            0  \n",
      "5            10                9                7        1     4            1  \n",
      "6            10                3                1        1     2            0  \n",
      "7             1                3                1        1     2            0  \n",
      "8             1                1                1        5     2            0  \n",
      "9             1                2                1        1     2            0  \n",
      "10            1                3                1        1     2            0  \n",
      "11            1                2                1        1     2            0  \n",
      "12            3                4                4        1     4            1  \n",
      "13            3                3                1        1     2            0  \n",
      "14            9                5                5        4     4            1  \n",
      "15            1                4                3        1     4            1  \n",
      "16            1                2                1        1     2            0  \n",
      "17            1                3                1        1     2            0  \n",
      "18           10                4                1        2     4            1  \n",
      "19            1                3                1        1     2            0  \n",
      "       Sample code number  Clump Thickness  Uniformity of Cell Size  \\\n",
      "count        6.830000e+02       683.000000               683.000000   \n",
      "mean         1.076720e+06         4.442167                 3.150805   \n",
      "std          6.206440e+05         2.820761                 3.065145   \n",
      "min          6.337500e+04         1.000000                 1.000000   \n",
      "25%          8.776170e+05         2.000000                 1.000000   \n",
      "50%          1.171795e+06         4.000000                 1.000000   \n",
      "75%          1.238705e+06         6.000000                 5.000000   \n",
      "max          1.345435e+07        10.000000                10.000000   \n",
      "\n",
      "       Uniformity of Cell Shape  Marginal Adhesion  \\\n",
      "count                683.000000         683.000000   \n",
      "mean                   3.215227           2.830161   \n",
      "std                    2.988581           2.864562   \n",
      "min                    1.000000           1.000000   \n",
      "25%                    1.000000           1.000000   \n",
      "50%                    1.000000           1.000000   \n",
      "75%                    5.000000           4.000000   \n",
      "max                   10.000000          10.000000   \n",
      "\n",
      "       Single Epithelial Cell Size  Bare Nuclei  Bland Chromatin  \\\n",
      "count                   683.000000   683.000000       683.000000   \n",
      "mean                      3.234261     3.544656         3.445095   \n",
      "std                       2.223085     3.643857         2.449697   \n",
      "min                       1.000000     1.000000         1.000000   \n",
      "25%                       2.000000     1.000000         2.000000   \n",
      "50%                       2.000000     1.000000         3.000000   \n",
      "75%                       4.000000     6.000000         5.000000   \n",
      "max                      10.000000    10.000000        10.000000   \n",
      "\n",
      "       Normal Nucleoli     Mitoses  class-value  \n",
      "count       683.000000  683.000000   683.000000  \n",
      "mean          2.869693    1.603221     0.349927  \n",
      "std           3.052666    1.732674     0.477296  \n",
      "min           1.000000    1.000000     0.000000  \n",
      "25%           1.000000    1.000000     0.000000  \n",
      "50%           1.000000    1.000000     0.000000  \n",
      "75%           4.000000    1.000000     1.000000  \n",
      "max          10.000000   10.000000     1.000000  \n",
      "class\n",
      "2    444\n",
      "4    239\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#vérifications\n",
    "print(dataset.shape)\n",
    "print(dataset.head(20))\n",
    "print(dataset.describe())\n",
    "print(dataset.groupby(\"class\").size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset[['Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size','Bare Nuclei', 'Bland Chromatin', 'Normal Nucleoli', 'Mitoses']]\n",
    "X = X.to_numpy()  #transform en numpy pour keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = dataset[['class-value']]  #Recupere colonne correspondante\n",
    "Y = Y.values #Recupere les valeurs\n",
    "Y = Y.astype(float) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data values\n",
      "[[ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [10]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [10]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Data values\")\n",
    "print(X)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = LabelEncoder()\n",
    "classes.fit(Y)\n",
    "classes_Y = classes.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [10]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 6]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [10]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [10]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [10]\n",
      " [10]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 8]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [10]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 4]\n",
      " [ 7]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 8]\n",
      " [10]\n",
      " [ 8]\n",
      " [ 7]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 6]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 5]\n",
      " [ 9]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 8]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 6]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 2]\n",
      " [ 2]\n",
      " [ 4]\n",
      " [ 6]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 8]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [10]\n",
      " [ 4]\n",
      " [ 4]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 7]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 2]\n",
      " [10]\n",
      " [ 5]\n",
      " [ 5]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 3]\n",
      " [ 4]\n",
      " [ 1]\n",
      " [ 1]\n",
      " [ 5]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 3]\n",
      " [ 2]\n",
      " [ 5]\n",
      " [ 4]\n",
      " [ 4]]\n",
      "[[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "(683, 1)\n",
      "(683, 1)\n"
     ]
    }
   ],
   "source": [
    "#onehot_Y = np_utils.to_categorical(classes_Y)\n",
    "\n",
    "print(X)\n",
    "print(Y)\n",
    "\n",
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Création d'un modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10,input_dim=9 , activation =\"sigmoid\")) #10 neurones dans la couche après la couche d'entrée\n",
    "model.add(Dense(4,activation =\"sigmoid\")) #couche cachée \n",
    "model.add(Dense(1, activation=\"sigmoid\")) #dernière couche"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 10)                100       \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 44        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 149\n",
      "Trainable params: 149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "683/683 [==============================] - 0s 175us/step - loss: 0.6573 - accuracy: 0.7540\n",
      "Epoch 2/25\n",
      "683/683 [==============================] - 0s 137us/step - loss: 0.6216 - accuracy: 0.6530\n",
      "Epoch 3/25\n",
      "683/683 [==============================] - 0s 138us/step - loss: 0.5930 - accuracy: 0.6501\n",
      "Epoch 4/25\n",
      "683/683 [==============================] - 0s 129us/step - loss: 0.5644 - accuracy: 0.6501\n",
      "Epoch 5/25\n",
      "683/683 [==============================] - 0s 135us/step - loss: 0.5345 - accuracy: 0.6618\n",
      "Epoch 6/25\n",
      "683/683 [==============================] - 0s 140us/step - loss: 0.5029 - accuracy: 0.7291\n",
      "Epoch 7/25\n",
      "683/683 [==============================] - 0s 130us/step - loss: 0.4681 - accuracy: 0.8228\n",
      "Epoch 8/25\n",
      "683/683 [==============================] - 0s 155us/step - loss: 0.4318 - accuracy: 0.8829\n",
      "Epoch 9/25\n",
      "683/683 [==============================] - 0s 161us/step - loss: 0.3966 - accuracy: 0.9151\n",
      "Epoch 10/25\n",
      "683/683 [==============================] - 0s 129us/step - loss: 0.3641 - accuracy: 0.9356\n",
      "Epoch 11/25\n",
      "683/683 [==============================] - 0s 149us/step - loss: 0.3342 - accuracy: 0.9385\n",
      "Epoch 12/25\n",
      "683/683 [==============================] - 0s 172us/step - loss: 0.3089 - accuracy: 0.9414\n",
      "Epoch 13/25\n",
      "683/683 [==============================] - 0s 184us/step - loss: 0.2854 - accuracy: 0.9488\n",
      "Epoch 14/25\n",
      "683/683 [==============================] - 0s 165us/step - loss: 0.2661 - accuracy: 0.9546\n",
      "Epoch 15/25\n",
      "683/683 [==============================] - 0s 147us/step - loss: 0.2479 - accuracy: 0.9546\n",
      "Epoch 16/25\n",
      "683/683 [==============================] - 0s 190us/step - loss: 0.2331 - accuracy: 0.9531\n",
      "Epoch 17/25\n",
      "683/683 [==============================] - 0s 187us/step - loss: 0.2202 - accuracy: 0.9531\n",
      "Epoch 18/25\n",
      "683/683 [==============================] - 0s 178us/step - loss: 0.2083 - accuracy: 0.9546\n",
      "Epoch 19/25\n",
      "683/683 [==============================] - 0s 126us/step - loss: 0.1989 - accuracy: 0.9575\n",
      "Epoch 20/25\n",
      "683/683 [==============================] - 0s 145us/step - loss: 0.1895 - accuracy: 0.9605\n",
      "Epoch 21/25\n",
      "683/683 [==============================] - 0s 137us/step - loss: 0.1811 - accuracy: 0.9590\n",
      "Epoch 22/25\n",
      "683/683 [==============================] - 0s 135us/step - loss: 0.1739 - accuracy: 0.9649\n",
      "Epoch 23/25\n",
      "683/683 [==============================] - 0s 216us/step - loss: 0.1672 - accuracy: 0.9619\n",
      "Epoch 24/25\n",
      "683/683 [==============================] - 0s 201us/step - loss: 0.1621 - accuracy: 0.9634\n",
      "Epoch 25/25\n",
      "683/683 [==============================] - 0s 185us/step - loss: 0.1561 - accuracy: 0.9619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8ae772a910>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=25, batch_size = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Evaluation du modele"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "683/683 [==============================] - 0s 13us/step\n",
      "accuracy 96.48609161376953\n",
      "loss 0.15254346861628104\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X,Y) \n",
    "print(\"accuracy\",(evaluation[1]*100))\n",
    "print(\"loss\",(evaluation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw prediction\n",
      "[0.03915527]\n",
      "[0.8558012]\n",
      "[0.04551044]\n",
      "[0.8764819]\n",
      "[0.04160658]\n",
      "[0.8783092]\n",
      "[0.7468401]\n",
      "[0.04250407]\n",
      "[0.0405257]\n",
      "[0.05606714]\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X)\n",
    "print(\"raw prediction\")\n",
    "for i in range(10):\n",
    "\tprint(predictions[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rounded row predictions\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "rounded = [round(x[0]) for x in predictions]\n",
    "print('Rounded row predictions')\n",
    "for i in range(10):\n",
    "    print(rounded[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 1, 1, 2, 1, 3, 1, 1] => 0 (expected 0)[D=-0.039155]\n",
      "[5, 4, 4, 5, 7, 10, 3, 2, 1] => 0 (expected 0)[D=-0.855801]\n",
      "[3, 1, 1, 1, 2, 2, 3, 1, 1] => 0 (expected 0)[D=-0.045510]\n",
      "[6, 8, 8, 1, 3, 4, 3, 7, 1] => 0 (expected 0)[D=-0.876482]\n",
      "[4, 1, 1, 3, 2, 1, 3, 1, 1] => 0 (expected 0)[D=-0.041607]\n",
      "[8, 10, 10, 8, 7, 10, 9, 7, 1] => 0 (expected 1)[D=0.121691]\n",
      "[1, 1, 1, 1, 2, 10, 3, 1, 1] => 0 (expected 0)[D=-0.746840]\n",
      "[2, 1, 2, 1, 2, 1, 3, 1, 1] => 0 (expected 0)[D=-0.042504]\n",
      "[2, 1, 1, 1, 2, 1, 1, 1, 5] => 0 (expected 0)[D=-0.040526]\n",
      "[4, 2, 1, 1, 2, 1, 2, 1, 1] => 0 (expected 0)[D=-0.056067]\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    distance = Y[i]-predictions[i]\n",
    "    print(\"%s => %d (expected %d)[D=%f]\"% (X[i].tolist(), predictions[i], Y[i], distance  ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_dim=9 , activation =\"sigmoid\")) #10 neurones dans la couche après la couche d'entrée\n",
    "    model.add(Dense(4,activation =\"sigmoid\")) #couche cachée \n",
    "    model.add(Dense(1, activation=\"sigmoid\")) #dernière couche\n",
    "    model.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.95029241 0.97067451]\n",
      "96.05%( 1%)\n"
     ]
    }
   ],
   "source": [
    "training = KerasClassifier(build_fn=my_model,epochs=200,batch_size=10,verbose=0)\n",
    "kfold = KFold(n_splits=2, shuffle=True)\n",
    "cv_result = cross_val_score(training,X,Y,cv=kfold)\n",
    "print(cv_result)\n",
    "print(\"%.2f%%(%2d%%)\"%(cv_result.mean()*100, cv_result.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Multiclasse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le one hot encoding est important pour le multiclasse.\n",
    "On va changer la fonction de loss (en categorical cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_dim=9 , activation =\"sigmoid\")) #10 neurones dans la couche après la couche d'entrée\n",
    "    model.add(Dense(4,activation =\"sigmoid\")) #couche cachée \n",
    "    model.add(Dense(2, activation=\"sigmoid\")) #dernière couche\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "classes = LabelEncoder()\n",
    "classes.fit(Y)\n",
    "classes_Y = classes.transform(Y)\n",
    "onehot_Y = np_utils.to_categorical(classes_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_model = other_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "683/683 [==============================] - 0s 685us/step - loss: 0.6993 - accuracy: 0.4495\n",
      "Epoch 2/25\n",
      "683/683 [==============================] - 0s 176us/step - loss: 0.6468 - accuracy: 0.8155\n",
      "Epoch 3/25\n",
      "683/683 [==============================] - 0s 157us/step - loss: 0.6114 - accuracy: 0.7101\n",
      "Epoch 4/25\n",
      "683/683 [==============================] - 0s 151us/step - loss: 0.5825 - accuracy: 0.6706\n",
      "Epoch 5/25\n",
      "683/683 [==============================] - 0s 151us/step - loss: 0.5544 - accuracy: 0.6881\n",
      "Epoch 6/25\n",
      "683/683 [==============================] - 0s 164us/step - loss: 0.5244 - accuracy: 0.7335\n",
      "Epoch 7/25\n",
      "683/683 [==============================] - 0s 168us/step - loss: 0.4912 - accuracy: 0.8287\n",
      "Epoch 8/25\n",
      "683/683 [==============================] - 0s 166us/step - loss: 0.4542 - accuracy: 0.8799\n",
      "Epoch 9/25\n",
      "683/683 [==============================] - 0s 160us/step - loss: 0.4150 - accuracy: 0.9107\n",
      "Epoch 10/25\n",
      "683/683 [==============================] - 0s 173us/step - loss: 0.3763 - accuracy: 0.9312\n",
      "Epoch 11/25\n",
      "683/683 [==============================] - 0s 173us/step - loss: 0.3382 - accuracy: 0.9473\n",
      "Epoch 12/25\n",
      "683/683 [==============================] - 0s 163us/step - loss: 0.3038 - accuracy: 0.9517\n",
      "Epoch 13/25\n",
      "683/683 [==============================] - 0s 169us/step - loss: 0.2751 - accuracy: 0.9575\n",
      "Epoch 14/25\n",
      "683/683 [==============================] - 0s 178us/step - loss: 0.2494 - accuracy: 0.9561\n",
      "Epoch 15/25\n",
      "683/683 [==============================] - 0s 171us/step - loss: 0.2288 - accuracy: 0.9575\n",
      "Epoch 16/25\n",
      "683/683 [==============================] - 0s 162us/step - loss: 0.2128 - accuracy: 0.9575\n",
      "Epoch 17/25\n",
      "683/683 [==============================] - 0s 170us/step - loss: 0.1964 - accuracy: 0.9619\n",
      "Epoch 18/25\n",
      "683/683 [==============================] - 0s 181us/step - loss: 0.1849 - accuracy: 0.9649\n",
      "Epoch 19/25\n",
      "683/683 [==============================] - 0s 167us/step - loss: 0.1744 - accuracy: 0.9634\n",
      "Epoch 20/25\n",
      "683/683 [==============================] - 0s 176us/step - loss: 0.1644 - accuracy: 0.9649\n",
      "Epoch 21/25\n",
      "683/683 [==============================] - 0s 158us/step - loss: 0.1576 - accuracy: 0.9619\n",
      "Epoch 22/25\n",
      "683/683 [==============================] - 0s 180us/step - loss: 0.1512 - accuracy: 0.9649\n",
      "Epoch 23/25\n",
      "683/683 [==============================] - 0s 209us/step - loss: 0.1465 - accuracy: 0.9663\n",
      "Epoch 24/25\n",
      "683/683 [==============================] - 0s 247us/step - loss: 0.1405 - accuracy: 0.9649\n",
      "Epoch 25/25\n",
      "683/683 [==============================] - 0s 211us/step - loss: 0.1356 - accuracy: 0.9649\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8a8a11ba10>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_model.fit(X,onehot_Y,epochs=25, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__call__() missing 1 required positional argument: 'inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-119-69e3f3a5941f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuild_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mother_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mkfold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcv_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0monehot_Y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkfold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%.2f%%(%2d%%)\"\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcv_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv_result\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __call__() missing 1 required positional argument: 'inputs'"
     ]
    }
   ],
   "source": [
    "training = KerasClassifier(build_fn=other_model(),epochs=200,batch_size=10,verbose=0)\n",
    "kfold = KFold(n_splits=2, shuffle=True)\n",
    "cv_result = cross_val_score(training,X,onehot_Y,cv=kfold)\n",
    "print(cv_result)\n",
    "print(\"%.2f%%(%2d%%)\"%(cv_result.mean()*100, cv_result.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Nouveau jeu de données: les iris"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4 descripteurs donc 4 neurones dans la couche d'entrée.\n",
    "3 classes donc 3 neurones dans la couche de sortie;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [1,2,3,4,\"race\"]\n",
    "iris = pd.read_csv(\"../data/iris.data\", names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.0\n",
       "1      0.0\n",
       "2      0.0\n",
       "3      0.0\n",
       "4      0.0\n",
       "      ... \n",
       "145    2.0\n",
       "146    2.0\n",
       "147    2.0\n",
       "148    2.0\n",
       "149    2.0\n",
       "Name: race-value, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris[\"race\"] = iris[\"race\"].astype(\"category\")\n",
    "iris[\"race-value\"] = iris[\"race\"].cat.codes\n",
    "#conversion en float\n",
    "iris[\"race-value\"].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris[[1,2,3,4]]\n",
    "X = X.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = iris[['race-value']]  #Recupere colonne correspondante\n",
    "Y = Y.values #Recupere les valeurs\n",
    "Y = Y.astype(float) #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/sdv/m2bi/ajaquaniello/.conda/envs/deep_learning/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "classes = LabelEncoder()\n",
    "classes.fit(Y)\n",
    "classes_Y = classes.transform(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_Y = np_utils.to_categorical(classes_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iris_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_dim = 4 , activation =\"sigmoid\")) #10 neurones dans la couche après la couche d'entrée\n",
    "    model.add(Dense(5,activation =\"sigmoid\")) #couche cachée \n",
    "    model.add(Dense(3, activation=\"sigmoid\")) #dernière couche\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_model = iris_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "150/150 [==============================] - 0s 3ms/step - loss: 1.1871 - accuracy: 0.3333\n",
      "Epoch 2/100\n",
      "150/150 [==============================] - 0s 265us/step - loss: 1.1727 - accuracy: 0.3333\n",
      "Epoch 3/100\n",
      "150/150 [==============================] - 0s 313us/step - loss: 1.1603 - accuracy: 0.3333\n",
      "Epoch 4/100\n",
      "150/150 [==============================] - 0s 327us/step - loss: 1.1504 - accuracy: 0.3333\n",
      "Epoch 5/100\n",
      "150/150 [==============================] - 0s 211us/step - loss: 1.1410 - accuracy: 0.3333\n",
      "Epoch 6/100\n",
      "150/150 [==============================] - 0s 294us/step - loss: 1.1337 - accuracy: 0.3333\n",
      "Epoch 7/100\n",
      "150/150 [==============================] - 0s 248us/step - loss: 1.1276 - accuracy: 0.3333\n",
      "Epoch 8/100\n",
      "150/150 [==============================] - 0s 224us/step - loss: 1.1221 - accuracy: 0.3333\n",
      "Epoch 9/100\n",
      "150/150 [==============================] - 0s 190us/step - loss: 1.1167 - accuracy: 0.3400\n",
      "Epoch 10/100\n",
      "150/150 [==============================] - 0s 197us/step - loss: 1.1124 - accuracy: 0.4267\n",
      "Epoch 11/100\n",
      "150/150 [==============================] - 0s 177us/step - loss: 1.1076 - accuracy: 0.6067\n",
      "Epoch 12/100\n",
      "150/150 [==============================] - 0s 215us/step - loss: 1.1032 - accuracy: 0.6533\n",
      "Epoch 13/100\n",
      "150/150 [==============================] - 0s 193us/step - loss: 1.0974 - accuracy: 0.6667\n",
      "Epoch 14/100\n",
      "150/150 [==============================] - 0s 280us/step - loss: 1.0911 - accuracy: 0.6667\n",
      "Epoch 15/100\n",
      "150/150 [==============================] - 0s 212us/step - loss: 1.0850 - accuracy: 0.6600\n",
      "Epoch 16/100\n",
      "150/150 [==============================] - 0s 229us/step - loss: 1.0782 - accuracy: 0.6600\n",
      "Epoch 17/100\n",
      "150/150 [==============================] - 0s 198us/step - loss: 1.0729 - accuracy: 0.6600\n",
      "Epoch 18/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 1.0683 - accuracy: 0.6533\n",
      "Epoch 19/100\n",
      "150/150 [==============================] - 0s 251us/step - loss: 1.0632 - accuracy: 0.6600\n",
      "Epoch 20/100\n",
      "150/150 [==============================] - 0s 208us/step - loss: 1.0588 - accuracy: 0.6667\n",
      "Epoch 21/100\n",
      "150/150 [==============================] - 0s 178us/step - loss: 1.0539 - accuracy: 0.6667\n",
      "Epoch 22/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 1.0489 - accuracy: 0.6667\n",
      "Epoch 23/100\n",
      "150/150 [==============================] - 0s 265us/step - loss: 1.0438 - accuracy: 0.6667\n",
      "Epoch 24/100\n",
      "150/150 [==============================] - 0s 207us/step - loss: 1.0382 - accuracy: 0.6667\n",
      "Epoch 25/100\n",
      "150/150 [==============================] - 0s 323us/step - loss: 1.0322 - accuracy: 0.6667\n",
      "Epoch 26/100\n",
      "150/150 [==============================] - 0s 235us/step - loss: 1.0260 - accuracy: 0.6667\n",
      "Epoch 27/100\n",
      "150/150 [==============================] - 0s 199us/step - loss: 1.0195 - accuracy: 0.6667\n",
      "Epoch 28/100\n",
      "150/150 [==============================] - 0s 222us/step - loss: 1.0124 - accuracy: 0.6667\n",
      "Epoch 29/100\n",
      "150/150 [==============================] - 0s 271us/step - loss: 1.0046 - accuracy: 0.6667\n",
      "Epoch 30/100\n",
      "150/150 [==============================] - 0s 263us/step - loss: 0.9969 - accuracy: 0.6667\n",
      "Epoch 31/100\n",
      "150/150 [==============================] - 0s 268us/step - loss: 0.9884 - accuracy: 0.6667\n",
      "Epoch 32/100\n",
      "150/150 [==============================] - 0s 268us/step - loss: 0.9791 - accuracy: 0.6667\n",
      "Epoch 33/100\n",
      "150/150 [==============================] - 0s 187us/step - loss: 0.9697 - accuracy: 0.6667\n",
      "Epoch 34/100\n",
      "150/150 [==============================] - 0s 184us/step - loss: 0.9597 - accuracy: 0.6667\n",
      "Epoch 35/100\n",
      "150/150 [==============================] - 0s 192us/step - loss: 0.9488 - accuracy: 0.6667\n",
      "Epoch 36/100\n",
      "150/150 [==============================] - 0s 203us/step - loss: 0.9377 - accuracy: 0.6667\n",
      "Epoch 37/100\n",
      "150/150 [==============================] - 0s 264us/step - loss: 0.9261 - accuracy: 0.6667\n",
      "Epoch 38/100\n",
      "150/150 [==============================] - 0s 205us/step - loss: 0.9143 - accuracy: 0.6667\n",
      "Epoch 39/100\n",
      "150/150 [==============================] - 0s 203us/step - loss: 0.9021 - accuracy: 0.6667\n",
      "Epoch 40/100\n",
      "150/150 [==============================] - 0s 208us/step - loss: 0.8899 - accuracy: 0.6667\n",
      "Epoch 41/100\n",
      "150/150 [==============================] - 0s 180us/step - loss: 0.8770 - accuracy: 0.6667\n",
      "Epoch 42/100\n",
      "150/150 [==============================] - 0s 192us/step - loss: 0.8635 - accuracy: 0.6667\n",
      "Epoch 43/100\n",
      "150/150 [==============================] - 0s 257us/step - loss: 0.8496 - accuracy: 0.6667\n",
      "Epoch 44/100\n",
      "150/150 [==============================] - 0s 215us/step - loss: 0.8354 - accuracy: 0.6667\n",
      "Epoch 45/100\n",
      "150/150 [==============================] - 0s 281us/step - loss: 0.8214 - accuracy: 0.6667\n",
      "Epoch 46/100\n",
      "150/150 [==============================] - 0s 217us/step - loss: 0.8076 - accuracy: 0.6667\n",
      "Epoch 47/100\n",
      "150/150 [==============================] - 0s 203us/step - loss: 0.7944 - accuracy: 0.6667\n",
      "Epoch 48/100\n",
      "150/150 [==============================] - 0s 257us/step - loss: 0.7815 - accuracy: 0.6667\n",
      "Epoch 49/100\n",
      "150/150 [==============================] - 0s 194us/step - loss: 0.7689 - accuracy: 0.6667\n",
      "Epoch 50/100\n",
      "150/150 [==============================] - 0s 213us/step - loss: 0.7571 - accuracy: 0.6667\n",
      "Epoch 51/100\n",
      "150/150 [==============================] - 0s 188us/step - loss: 0.7458 - accuracy: 0.7200\n",
      "Epoch 52/100\n",
      "150/150 [==============================] - 0s 190us/step - loss: 0.7348 - accuracy: 0.9400\n",
      "Epoch 53/100\n",
      "150/150 [==============================] - 0s 320us/step - loss: 0.7249 - accuracy: 0.9067\n",
      "Epoch 54/100\n",
      "150/150 [==============================] - 0s 255us/step - loss: 0.7144 - accuracy: 0.9467\n",
      "Epoch 55/100\n",
      "150/150 [==============================] - 0s 267us/step - loss: 0.7048 - accuracy: 0.9733\n",
      "Epoch 56/100\n",
      "150/150 [==============================] - 0s 238us/step - loss: 0.6955 - accuracy: 0.9600\n",
      "Epoch 57/100\n",
      "150/150 [==============================] - 0s 187us/step - loss: 0.6866 - accuracy: 0.8667\n",
      "Epoch 58/100\n",
      "150/150 [==============================] - 0s 274us/step - loss: 0.6783 - accuracy: 0.8467\n",
      "Epoch 59/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.6703 - accuracy: 0.7667\n",
      "Epoch 60/100\n",
      "150/150 [==============================] - 0s 208us/step - loss: 0.6624 - accuracy: 0.7933\n",
      "Epoch 61/100\n",
      "150/150 [==============================] - 0s 219us/step - loss: 0.6553 - accuracy: 0.7800\n",
      "Epoch 62/100\n",
      "150/150 [==============================] - 0s 198us/step - loss: 0.6480 - accuracy: 0.7533\n",
      "Epoch 63/100\n",
      "150/150 [==============================] - 0s 178us/step - loss: 0.6416 - accuracy: 0.7333\n",
      "Epoch 64/100\n",
      "150/150 [==============================] - 0s 215us/step - loss: 0.6349 - accuracy: 0.7333\n",
      "Epoch 65/100\n",
      "150/150 [==============================] - 0s 236us/step - loss: 0.6290 - accuracy: 0.7600\n",
      "Epoch 66/100\n",
      "150/150 [==============================] - 0s 254us/step - loss: 0.6230 - accuracy: 0.7867\n",
      "Epoch 67/100\n",
      "150/150 [==============================] - 0s 201us/step - loss: 0.6174 - accuracy: 0.7533\n",
      "Epoch 68/100\n",
      "150/150 [==============================] - 0s 205us/step - loss: 0.6121 - accuracy: 0.7333\n",
      "Epoch 69/100\n",
      "150/150 [==============================] - 0s 252us/step - loss: 0.6071 - accuracy: 0.7600\n",
      "Epoch 70/100\n",
      "150/150 [==============================] - 0s 221us/step - loss: 0.6021 - accuracy: 0.7867\n",
      "Epoch 71/100\n",
      "150/150 [==============================] - 0s 266us/step - loss: 0.5975 - accuracy: 0.7733\n",
      "Epoch 72/100\n",
      "150/150 [==============================] - 0s 186us/step - loss: 0.5932 - accuracy: 0.7733\n",
      "Epoch 73/100\n",
      "150/150 [==============================] - 0s 209us/step - loss: 0.5890 - accuracy: 0.7867\n",
      "Epoch 74/100\n",
      "150/150 [==============================] - 0s 287us/step - loss: 0.5848 - accuracy: 0.7600\n",
      "Epoch 75/100\n",
      "150/150 [==============================] - 0s 179us/step - loss: 0.5810 - accuracy: 0.8000\n",
      "Epoch 76/100\n",
      "150/150 [==============================] - 0s 220us/step - loss: 0.5774 - accuracy: 0.8533\n",
      "Epoch 77/100\n",
      "150/150 [==============================] - 0s 188us/step - loss: 0.5741 - accuracy: 0.7533\n",
      "Epoch 78/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.5705 - accuracy: 0.8000\n",
      "Epoch 79/100\n",
      "150/150 [==============================] - 0s 284us/step - loss: 0.5672 - accuracy: 0.8333\n",
      "Epoch 80/100\n",
      "150/150 [==============================] - 0s 227us/step - loss: 0.5642 - accuracy: 0.8400\n",
      "Epoch 81/100\n",
      "150/150 [==============================] - 0s 233us/step - loss: 0.5611 - accuracy: 0.8533\n",
      "Epoch 82/100\n",
      "150/150 [==============================] - 0s 212us/step - loss: 0.5584 - accuracy: 0.8733\n",
      "Epoch 83/100\n",
      "150/150 [==============================] - 0s 251us/step - loss: 0.5555 - accuracy: 0.8800\n",
      "Epoch 84/100\n",
      "150/150 [==============================] - 0s 217us/step - loss: 0.5530 - accuracy: 0.8533\n",
      "Epoch 85/100\n",
      "150/150 [==============================] - 0s 270us/step - loss: 0.5504 - accuracy: 0.8800\n",
      "Epoch 86/100\n",
      "150/150 [==============================] - 0s 302us/step - loss: 0.5481 - accuracy: 0.8800\n",
      "Epoch 87/100\n",
      "150/150 [==============================] - 0s 337us/step - loss: 0.5457 - accuracy: 0.8667\n",
      "Epoch 88/100\n",
      "150/150 [==============================] - 0s 223us/step - loss: 0.5436 - accuracy: 0.8467\n",
      "Epoch 89/100\n",
      "150/150 [==============================] - 0s 194us/step - loss: 0.5415 - accuracy: 0.8200\n",
      "Epoch 90/100\n",
      "150/150 [==============================] - 0s 202us/step - loss: 0.5392 - accuracy: 0.8667\n",
      "Epoch 91/100\n",
      "150/150 [==============================] - 0s 269us/step - loss: 0.5375 - accuracy: 0.9400\n",
      "Epoch 92/100\n",
      "150/150 [==============================] - 0s 250us/step - loss: 0.5355 - accuracy: 0.7467\n",
      "Epoch 93/100\n",
      "150/150 [==============================] - 0s 286us/step - loss: 0.5337 - accuracy: 0.8467\n",
      "Epoch 94/100\n",
      "150/150 [==============================] - 0s 301us/step - loss: 0.5320 - accuracy: 0.7800\n",
      "Epoch 95/100\n",
      "150/150 [==============================] - 0s 277us/step - loss: 0.5301 - accuracy: 0.6667\n",
      "Epoch 96/100\n",
      "150/150 [==============================] - 0s 250us/step - loss: 0.5284 - accuracy: 0.7200\n",
      "Epoch 97/100\n",
      "150/150 [==============================] - 0s 289us/step - loss: 0.5268 - accuracy: 0.6800\n",
      "Epoch 98/100\n",
      "150/150 [==============================] - 0s 239us/step - loss: 0.5254 - accuracy: 0.6667\n",
      "Epoch 99/100\n",
      "150/150 [==============================] - 0s 311us/step - loss: 0.5239 - accuracy: 0.7067\n",
      "Epoch 100/100\n",
      "150/150 [==============================] - 0s 285us/step - loss: 0.5223 - accuracy: 0.6733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f8a7a97cf10>"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris_model.fit(X,one_hot_Y,epochs=100, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def other_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(10,input_dim = 4 , activation =\"sigmoid\")) #10 neurones dans la couche après la couche d'entrée\n",
    "    model.add(Dense(5,activation =\"sigmoid\")) #couche cachée \n",
    "    model.add(Dense(3, activation=\"sigmoid\")) #dernière couche\n",
    "    model.compile(optimizer=\"adam\",loss=\"categorical_crossentropy\",metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross val: [1.         1.         0.93333334 1.         1.         1.\n",
      " 1.         0.93333334 1.         0.93333334]\n",
      "98.00%( 3%)\n"
     ]
    }
   ],
   "source": [
    "training = KerasClassifier(build_fn = other_model,epochs=200,batch_size=10,verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "cv_result = cross_val_score(training,X,one_hot_Y,cv=kfold)\n",
    "print(\"Cross val: {}\".format(cv_result))\n",
    "print(\"%.2f%%(%2d%%)\"%(cv_result.mean()*100, cv_result.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
